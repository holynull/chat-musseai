import os
from typing import Annotated, Optional, cast
from enhanced_router_tools import EnhancedRouterTools
from typing_extensions import TypedDict
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import (
    RunnableConfig,
)
from langchain_core.language_models import BaseChatModel
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.prompts import (
    SystemMessagePromptTemplate,
)
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.utils.runnable import RunnableCallable
from langgraph.types import Command
from tools.tools_agent_router import tools as tools_router

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    max_tokens=4096,
    temperature=0.9,
    # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
    streaming=True,
    stream_usage=True,
    verbose=True,
)

# llm = ChatAnthropic(
#     model="claude-3-5-sonnet-20241022",
#     max_tokens=4096,
#     temperature=0.9,
#     # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#     streaming=True,
#     stream_usage=True,
#     verbose=True,
# ).configurable_alternatives(  # This gives this field an id
#     # When configuring the end runnable, we can then use this id to configure this field
#     ConfigurableField(id="llm"),
#     # default_key="openai_gpt_4_turbo_preview",
#     default_key="anthropic_claude_3_5_sonnet",
#     anthropic_claude_3_opus=ChatAnthropic(
#         model="claude-3-opus-20240229",
#         # max_tokens=,
#         temperature=0.9,
#         # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#         streaming=True,
#         verbose=True,
#     ),
#     anthropic_claude_3_7_sonnet=ChatAnthropic(
#         model="claude-3-7-sonnet-20250219",
#         # max_tokens=,
#         temperature=0.9,
#         # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#         streaming=True,
#         verbose=True,
#     ),
#     anthropic_claude_4_sonnet=ChatAnthropic(
#         model="claude-sonnet-4-20250514",
#         # max_tokens=,
#         temperature=0.9,
#         # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#         streaming=True,
#         verbose=True,
#     ),
# )


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
    wallet_is_connected: bool
    chain_id: int
    wallet_address: str
    time_zone: str
    llm: str
    user_id: str


graph_builder = StateGraph(State)


from prompts.prompt_chatbot import system_prompt

system_template = SystemMessagePromptTemplate.from_template(system_prompt)

from tools.tools_swap import tools as tools_swap
from tools.tools_wallet import tools as tools_wallet
from tools.tools_search import tools as tools_search
from tools.tools_quote import tools as tools_quote
from tools.tools_image import tools as tools_image
from tools.tools_infura import tools as tools_infura
from tools.tools_solana import tools as tools_solana
from tools.tools_crypto_portfolios import tools as tools_crypto_portfolios

ROUTE_MAPPING = {
    "route_to_swap_agent": {
        "node_name": "node_swap_graph",
        "tools": [t.get_name() for t in tools_swap],
    },
    "route_to_wallet_agent": {
        "node_name": "node_wallet_graph",
        "tools": [t.get_name() for t in tools_wallet],
    },
    "route_to_search_agent": {
        "node_name": "node_search_graph",
        "tools": [t.get_name() for t in tools_search],
    },
    "route_to_quote_agent": {
        "node_name": "node_quote_graph",
        "tools": [t.get_name() for t in tools_quote],
    },
    "route_to_image_agent": {
        "node_name": "node_image_graph",
        "tools": [t.get_name() for t in tools_image],
    },
    "route_to_infura_agent": {
        "node_name": "node_infura_graph",
        "tools": [t.get_name() for t in tools_infura],
    },
    "route_to_solana_agent": {
        "node_name": "node_solana_graph",
        "tools": [t.get_name() for t in tools_solana],
    },
    "route_to_crypto_portfolios_agent": {
        "node_name": "node_crypto_portfolios_graph",
        "tools": [t.get_name() for t in tools_crypto_portfolios],
    },
}


def build_tool_to_node_mapping():
    """构建工具名到节点名的映射"""
    mapping = {}
    for route_key, route_info in ROUTE_MAPPING.items():
        if isinstance(route_info, dict) and "tools" in route_info:
            for tool in route_info["tools"]:
                mapping[tool] = route_info["node_name"]
    return mapping


TOOL_TO_NODE_MAPPING = build_tool_to_node_mapping()


def call_model(state: State, config: RunnableConfig):
    raise NotImplementedError()


async def acall_model(state: State, config: RunnableConfig):
    llm_configed = (
        cast(BaseChatModel, llm)
        .bind_tools(tools_router)
        .with_config(
            {
                "configurable": {"llm": state["llm"]},
            }
        )
    )

    system_message = system_template.format_messages(
        wallet_is_connected=state["wallet_is_connected"],
        chain_id=state["chain_id"],
        wallet_address=state["wallet_address"],
        time_zone=state["time_zone"],
    )
    _response = await llm_configed.ainvoke(system_message + state["messages"])
    response = cast(AIMessage, _response)
    next_node = await aget_next_node(state["messages"] + [response])
    if next_node:
        return Command(
            goto=next_node,
            update=state,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


router_tools = EnhancedRouterTools(tools=tools_router, name="node_tools_router")

node_llm = RunnableCallable(call_model, acall_model, name="node_llm_musseai")


from graphs.graph_wallet import graph as wallet_graph
from graphs.graph_search import graph as search_graph
from graphs.graph_swap import graph as swap_graph
from graphs.graph_quote import graph as quote_graph
from graphs.graph_image import graph as image_graph
from graphs.graph_infura import graph as infura_graph
from graphs.graph_solana import graph as solana_graph
from graphs.graph_crypto_portfolios import graph as crypto_portfolios_graph


def _extract_message_content(message: AIMessage) -> Optional[str]:
    """安全提取消息内容"""
    try:
        if isinstance(message.content, str):
            return message.content
        elif isinstance(message.content, list) and len(message.content) > 0:
            first_content = message.content[0]
            if isinstance(first_content, dict) and "text" in first_content:
                return first_content["text"]
            elif isinstance(first_content, str):
                return first_content
        return None
    except (IndexError, KeyError, AttributeError):
        return None


def _build_routing_prompt(content: str) -> HumanMessage:
    """构建路由判断提示"""
    prompt_text = f"""
    Based on the following AI response, determine if it needs to be routed to a specialized agent:
    
    AI Response: {content}
    
    Analyze if the response:
    1. Mentions specific actions that require specialized tools
    2. Contains incomplete information that needs additional processing
    3. References functionality handled by specific agents
    
    If routing is needed, call the appropriate routing tool. Otherwise, do not call any tools.
    """
    return HumanMessage(content=prompt_text)


def _parse_routing_response(response: AIMessage) -> Optional[str]:
    """解析路由响应"""
    try:
        if not response.tool_calls:
            return None

        tool_call = response.tool_calls[0]
        tool_name = tool_call.get("name")

        if tool_name in ROUTE_MAPPING:
            return ROUTE_MAPPING[tool_name]["node_name"]
        elif tool_name in TOOL_TO_NODE_MAPPING:
            return TOOL_TO_NODE_MAPPING[tool_name]

        return None
    except (IndexError, KeyError, AttributeError):
        return None


async def _intelligent_routing(ai_message: AIMessage) -> Optional[str]:
    """
    基于AI消息内容进行智能路由判断
    """
    try:
        # 安全获取消息内容
        content = _extract_message_content(ai_message)
        if not content:
            return None

        # 创建轻量级LLM用于路由判断
        router_llm = ChatAnthropic(
            model="claude-3-5-haiku-latest",
            temperature=0.1,
            max_tokens=100,  # 限制token数量
        )

        # 构建路由判断提示
        routing_prompt = _build_routing_prompt(content)

        # 绑定路由工具
        llm_with_tools = router_llm.bind_tools(tools_router)

        # 获取路由决策
        response = await llm_with_tools.ainvoke([routing_prompt])

        # 解析路由结果
        return _parse_routing_response(response)

    except Exception as e:
        print(f"Error in intelligent routing: {e}")
        return None


async def aget_next_node(messages: list) -> Optional[str]:
    """
    根据消息历史确定下一个路由节点

    Args:
        messages: 消息历史列表

    Returns:
        下一个节点名称，如果不需要路由则返回None
    """
    try:
        # 获取最后一个AI消息
        last_ai_message = None
        for message in reversed(messages):
            if isinstance(message, AIMessage):
                last_ai_message = message
                break

        if not last_ai_message:
            return None

        # 检查是否有工具调用
        if hasattr(last_ai_message, "tool_calls") and last_ai_message.tool_calls:
            for tool_call in last_ai_message.tool_calls:
                tool_name = tool_call.get("name")
                if tool_name:
                    # 直接从路由映射获取节点
                    if tool_name in ROUTE_MAPPING:
                        return ROUTE_MAPPING[tool_name]["node_name"]
                    # 从工具到节点映射获取
                    elif tool_name in TOOL_TO_NODE_MAPPING:
                        return TOOL_TO_NODE_MAPPING[tool_name]

        # 如果没有工具调用，使用智能路由判断
        return await _intelligent_routing(last_ai_message)

    except Exception as e:
        print(f"Error in get_next_node: {e}")
        return None


async def node_swap_graph(state: State):
    _response = await swap_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_swap_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_wallet_graph(state: State):
    _response = await wallet_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_wallet_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_search_graph(state: State):
    _response = await search_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_search_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_quote_graph(state: State):
    _response = await quote_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_quote_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_image_graph(state: State):
    _response = await image_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_image_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_infura_graph(state: State):
    _response = await infura_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_infura_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_solana_graph(state: State):
    _response = await solana_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_solana_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_crypto_portfolios_graph(state: State):
    _response = await crypto_portfolios_graph.ainvoke(state)
    next_node = await aget_next_node(_response["messages"])
    if next_node and next_node != node_crypto_portfolios_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


graph_builder.add_node(node_llm.get_name(), node_llm)
# graph_builder.add_node(node_chatbot)
graph_builder.add_node(node_swap_graph)
graph_builder.add_node(node_wallet_graph)
graph_builder.add_node(node_search_graph)
graph_builder.add_node(node_quote_graph)
graph_builder.add_node(node_image_graph)
graph_builder.add_node(node_infura_graph)
graph_builder.add_node(node_solana_graph)
graph_builder.add_node(node_crypto_portfolios_graph)

# add edge
graph_builder.add_edge(START, node_llm.get_name())


from langgraph.store.memory import InMemoryStore
from langchain_ollama import OllamaEmbeddings
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()


in_memory_store = InMemoryStore(
    index={
        "embed": OllamaEmbeddings(model="llama3"),
        "dims": 1536,
    }
)

# graph = graph_builder.compile(checkpointer=memory, debug=False, store=in_memory_store)
graph = graph_builder.compile()
graph.name = "mussai_agent"
