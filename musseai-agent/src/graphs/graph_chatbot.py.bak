import os
from typing import Annotated, Literal, cast
from enhanced_router_tools import EnhancedRouterTools
from agent_config import AGENT_CONFIGS, get_agent_config
from typing_extensions import TypedDict

from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatPerplexity

from langchain_core.runnables import ConfigurableField
from langchain_core.runnables import (
    RunnableConfig,
)
from langchain_core.language_models import BaseChatModel

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage
from langchain_core.prompts import (
    SystemMessagePromptTemplate,
)
from langchain_core.messages import ToolMessage

from langchain_core.messages import AIMessage, HumanMessage
from langgraph.utils.runnable import RunnableCallable
from langgraph.types import Command

from tools.tools_agent_router import tools as tools_router, get_next_node

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    max_tokens=4096,
    temperature=0.9,
    # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
    streaming=True,
    stream_usage=True,
    verbose=True,
)

# llm = ChatAnthropic(
#     model="claude-3-5-sonnet-20241022",
#     max_tokens=4096,
#     temperature=0.9,
#     # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#     streaming=True,
#     stream_usage=True,
#     verbose=True,
# ).configurable_alternatives(  # This gives this field an id
#     # When configuring the end runnable, we can then use this id to configure this field
#     ConfigurableField(id="llm"),
#     # default_key="openai_gpt_4_turbo_preview",
#     default_key="anthropic_claude_3_5_sonnet",
#     anthropic_claude_3_opus=ChatAnthropic(
#         model="claude-3-opus-20240229",
#         # max_tokens=,
#         temperature=0.9,
#         # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#         streaming=True,
#         verbose=True,
#     ),
#     anthropic_claude_3_7_sonnet=ChatAnthropic(
#         model="claude-3-7-sonnet-20250219",
#         # max_tokens=,
#         temperature=0.9,
#         # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#         streaming=True,
#         verbose=True,
#     ),
#     anthropic_claude_4_sonnet=ChatAnthropic(
#         model="claude-sonnet-4-20250514",
#         # max_tokens=,
#         temperature=0.9,
#         # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
#         streaming=True,
#         verbose=True,
#     ),
# )


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
    wallet_is_connected: bool
    chain_id: int
    wallet_address: str
    time_zone: str
    llm: str
    user_id: str


graph_builder = StateGraph(State)


from prompts.prompt_chatbot import system_prompt

system_template = SystemMessagePromptTemplate.from_template(system_prompt)

from tools.tools_swap import tools as tools_swap
from tools.tools_wallet import tools as tools_wallet
from tools.tools_search import tools as tools_search
from tools.tools_quote import tools as tools_quote
from tools.tools_image import tools as tools_image
from tools.tools_infura import tools as tools_infura
from tools.tools_solana import tools as tools_solana
from tools.tools_crypto_portfolios import tools as tools_crypto_portfolios

ROUTE_MAPPING = {
    "route_to_swap_agent": {
        "node_name": "node_swap_graph",
        "tools": [t.get_name() for t in tools_swap],
    },
    "route_to_wallet_agent": {
        "node_name": "node_wallet_graph",
        "tools": [t.get_name() for t in tools_wallet],
    },
    "route_to_search_agent": {
        "node_name": "node_search_graph",
        "tools": [t.get_name() for t in tools_search],
    },
    "route_to_quote_agent": {
        "node_name": "node_quote_graph",
        "tools": [t.get_name() for t in tools_quote],
    },
    "route_to_image_agent": {
        "node_name": "node_image_graph",
        "tools": [t.get_name() for t in tools_image],
    },
    "route_to_infura_agent": {
        "node_name": "node_infura_graph",
        "tools": [t.get_name() for t in tools_infura],
    },
    "route_to_solana_agent": {
        "node_name": "node_solana_graph",
        "tools": [t.get_name() for t in tools_solana],
    },
    "route_to_crypto_portfolios_agent": {
        "node_name": "node_crypto_portfolios_graph",
        "tools": [t.get_name() for t in tools_crypto_portfolios],
    },
}


def build_tool_to_node_mapping():
    """构建工具名到节点名的映射"""
    mapping = {}
    for route_key, route_info in ROUTE_MAPPING.items():
        if isinstance(route_info, dict) and "tools" in route_info:
            for tool in route_info["tools"]:
                mapping[tool] = route_info["node_name"]
    return mapping


TOOL_TO_NODE_MAPPING = build_tool_to_node_mapping()


def call_model(state: State, config: RunnableConfig):
    next_node = None
    tool_call = None
    llm_configed = (
        cast(BaseChatModel, llm)
        .bind_tools(tools_router)
        .with_config(
            {
                "configurable": {"llm": state["llm"]},
            }
        )
    )

    system_message = system_template.format_messages(
        wallet_is_connected=state["wallet_is_connected"],
        chain_id=state["chain_id"],
        wallet_address=state["wallet_address"],
        time_zone=state["time_zone"],
    )
    _response = llm_configed.invoke(system_message + state["messages"])
    response = cast(AIMessage, _response)
    if response.tool_calls and len(response.tool_calls) > 0:
        tool_call = response.tool_calls[0]["name"]
    if tool_call and tool_call in ROUTE_MAPPING:
        next_node = ROUTE_MAPPING[tool_call]["node_name"]
    elif tool_call not in ROUTE_MAPPING:
        next_node = TOOL_TO_NODE_MAPPING.get(tool_call)
    # next_node = get_next_node_from_messages([_response], node_chatbot.__name__, None)
    if next_node:
        # if _response["messages"] and isinstance(_response["messages"][-1], AIMessage):
        #     _response["messages"].pop()
        return Command(
            goto=next_node,
            update=state,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def acall_model(state: State, config: RunnableConfig):
    next_node = None
    tool_call = None
    llm_configed = (
        cast(BaseChatModel, llm)
        .bind_tools(tools_router)
        .with_config(
            {
                "configurable": {"llm": state["llm"]},
            }
        )
    )

    system_message = system_template.format_messages(
        wallet_is_connected=state["wallet_is_connected"],
        chain_id=state["chain_id"],
        wallet_address=state["wallet_address"],
        time_zone=state["time_zone"],
    )
    _response = await llm_configed.ainvoke(system_message + state["messages"])
    response = cast(AIMessage, _response)
    if response.tool_calls and len(response.tool_calls) > 0:
        tool_call = response.tool_calls[0]["name"]
    if tool_call and tool_call in ROUTE_MAPPING:
        next_node = ROUTE_MAPPING[tool_call]["node_name"]
    elif tool_call not in ROUTE_MAPPING:
        next_node = TOOL_TO_NODE_MAPPING.get(tool_call)
    # next_node = get_next_node_from_messages([_response], node_chatbot.__name__, None)
    if next_node:
        # if _response["messages"] and isinstance(_response["messages"][-1], AIMessage):
        #     _response["messages"].pop()
        return Command(
            goto=next_node,
            update=state,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


router_tools = EnhancedRouterTools(tools=tools_router, name="node_tools_router")

node_llm = RunnableCallable(call_model, acall_model, name="node_llm_musseai")


from graphs.graph_wallet import graph as wallet_graph
from graphs.graph_search import graph as search_graph
from graphs.graph_swap import graph as swap_graph
from graphs.graph_quote import graph as quote_graph
from graphs.graph_image import graph as image_graph
from graphs.graph_infura import graph as infura_graph
from graphs.graph_solana import graph as solana_graph
from graphs.graph_crypto_portfolios import graph as crypto_portfolios_graph


async def get_next_node(messages: list):
    next_node = None
    tool_call = None
    _llm = ChatAnthropic(
        model="claude-3-5-haiku-latest",
        # max_tokens=,
        temperature=0.1,
        # anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
        streaming=True,
        verbose=True,
    )
    llm_bind_tools = _llm.bind_tools(tools_router)
    last_ai_message = None
    for message in reversed(messages):
        if isinstance(message, AIMessage):
            last_ai_message = message
            break
    if last_ai_message:
        humanMessage = HumanMessage(
            content=f"AI: {last_ai_message.content[0]['text']}\n\nBased on the above answer, is it necessary to route to other agents?"
        )
        _res = await llm_bind_tools.ainvoke([humanMessage])
        response = cast(AIMessage, _res)
        if response.tool_calls and len(response.tool_calls) > 0:
            tool_call = response.tool_calls[0]["name"]
        if tool_call and tool_call in ROUTE_MAPPING:
            next_node = ROUTE_MAPPING[tool_call]["node_name"]
        elif tool_call not in ROUTE_MAPPING:
            next_node = TOOL_TO_NODE_MAPPING.get(tool_call)
    return next_node


async def node_swap_graph(state: State):
    _response = await swap_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_swap_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_wallet_graph(state: State):
    _response = await wallet_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_wallet_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_search_graph(state: State):
    _response = await search_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_search_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_quote_graph(state: State):
    _response = await quote_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_quote_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_image_graph(state: State):
    _response = await image_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_image_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_infura_graph(state: State):
    _response = await infura_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_infura_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_solana_graph(state: State):
    _response = await solana_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_solana_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


async def node_crypto_portfolios_graph(state: State):
    _response = await crypto_portfolios_graph.ainvoke(state)
    next_node = await get_next_node(_response["messages"])
    if next_node and next_node != node_crypto_portfolios_graph.__name__:
        _response["messages"].pop()
        return Command(
            goto=next_node,
            update=_response,
        )
    else:
        return Command(
            goto=END,
            update=_response,
        )


graph_builder.add_node(node_llm.get_name(), node_llm)
# graph_builder.add_node(node_chatbot)
graph_builder.add_node(node_swap_graph)
graph_builder.add_node(node_wallet_graph)
graph_builder.add_node(node_search_graph)
graph_builder.add_node(node_quote_graph)
graph_builder.add_node(node_image_graph)
graph_builder.add_node(node_infura_graph)
graph_builder.add_node(node_solana_graph)
graph_builder.add_node(node_crypto_portfolios_graph)

# add edge
graph_builder.add_edge(START, node_llm.get_name())


from langgraph.store.memory import InMemoryStore
from langchain_ollama import OllamaEmbeddings
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()


in_memory_store = InMemoryStore(
    index={
        "embed": OllamaEmbeddings(model="llama3"),
        "dims": 1536,
    }
)

# graph = graph_builder.compile(checkpointer=memory, debug=False, store=in_memory_store)
graph = graph_builder.compile()
graph.name = "mussai_agent"
